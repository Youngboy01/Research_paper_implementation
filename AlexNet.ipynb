{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to implement AlexNet architecture from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class local_response_normalization(nn.Module):\n",
    "    def __init__(self,k=2,alpha=1e-4,n=5,beta=0.75):\n",
    "        super(local_response_normalization,self).__init__()\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.n = n\n",
    "        self.beta = beta\n",
    "    def forward(self,x):\n",
    "        N,C,H,W = x.shape # N is batch size, C is number of channels, H is height, W is width of the input # we are extracting dimensions of the input\n",
    "        #When a convolutional layer has C kernels(filters), it produces C output feature maps (channels) . So here N=C (no. of channels in layer)\n",
    "        summation = torch.zeros(x.size())# Creating a tensor of zeros with the same shape as x\n",
    "        for i in range(C):\n",
    "            for j in range(max(0,i-self.n//2),min(C-1,i+self.n//2)):\n",
    "                summation[:,i,:,:] += x[:,j,:,:]*x[:,j,:,:]#batches : , channels : , height : , width :\n",
    "        denom = (self.k+self.alpha*summation)**self.beta\n",
    "        return x/denom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of layers-\n",
    "1. Convolution layer\n",
    "2. ReLU activation\n",
    "3. Local response normalization\n",
    "4. Max pooling\n",
    "\n",
    "**\"The response-normalization layers follow the first and second convolutional layers.\"** is what the paper says"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self,num_classes=10):#Adjusted for CIFAR-10 dataset\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.lrn = local_response_normalization(k=2,alpha=1e-4,n=5,beta=0.75)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self.lrn,\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(96,256,5,padding=2),#since same convolution is used padding=filter-1/2 ie (5-1)/2 = 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            self.lrn,\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(256,384,3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,384,3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256,3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "        )#This ends our convolution part of the network\n",
    "        #Now we will define the fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(6*6*256,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096,num_classes)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        #x = nn.Flatten(x,1)#cretes problem in dropout because flatten layer is being passes directly to dropout layer, flatten is not tensor but layer hence cannot be passed to dropout layer\n",
    "        x = x.view(x.size(0),-1)#flattening the input tensor to a 2D tensor[batch_size, num_features]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),  # Resize the image to 227x227\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image. #the mean and standard deviation values used here are the ones used in the original AlexNet paper.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load the CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(train_dataset, range(0, 1000))  # Use only 1000 samples\n",
    "test_dataset = Subset(test_dataset, range(0, 200))\n",
    "#Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model, loss function and optimizer\n",
    "model = AlexNet(num_classes=10)  # CIFAR-10 has 10 classes\n",
    "#setup device agnostic code to run on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # Stochastic Gradient Descent optimizer\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Learning rate scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Training step\n",
    "def train(model, train_dataloader, optimizer, loss_fn, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        #track the loss and accuracy\n",
    "        running_loss += loss.item() # Accumulate loss\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    epoch_accuracy = correct / total * 100.0\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing step\n",
    "def test(model, test_dataloader, loss_fn, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation during testing\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Compute loss\n",
    "\n",
    "            running_loss += loss.item() # Accumulate loss\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    epoch_loss = running_loss / len(test_dataloader.dataset)\n",
    "    epoch_accuracy = correct / total * 100.0\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Training loop\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, train_dataloader, optimizer, loss_fn, device)\n",
    "    test_loss, test_accuracy = test(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] || Train Loss: {train_loss:.4f} || Train Accuracy: {train_accuracy:.2f}% || Test Loss: {test_loss:.4f} || Test Accuracy: {test_accuracy:.2f}%')\n",
    "    #scheduler.step()  # Update learning rate if using a scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
